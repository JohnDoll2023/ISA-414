{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project for ISA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Analysis: Score: -53  Positive Tweets: 23   Negative Tweets: 76   Total Tweets: 99\n",
      "{'sadness': 33, 'joy': 53, 'fear': 0, 'disgust': 6, 'anger': 7}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# Imports\n",
    "# ---------------------------------------------\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import tweepy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# CSV Files\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "# Reddit\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import numpy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# IBM Credentials for emotion\n",
    "# ---------------------------------------------\n",
    "ibm_url  = \"https://api.us-south.natural-language-understanding.watson.cloud.ibm.com/instances/d6058b89-d39d-464c-a756-50658dd3124b\"\n",
    "ibm_api_key    = \"LeNnOkIAOfA5VBWG6B7luAFzEJn4Q-z24AqSrzHaAGuG\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Twitter API\n",
    "# ---------------------------------------------\n",
    "consumer_key = \"zs0yFmnhHnfjnIS3eOXU9AtJU\"\n",
    "consumer_secret = \"dbsGwYqw4jxgpiuY1kQH5Yf4dECdk9JCsJVAKaYH1ExUAoVz9B\"\n",
    "access_token = \"1357013845647101955-t5k1nAOzGIKb6oohfw99l7F2aOVIJp\"\n",
    "access_token_secret = \"DkgJun3wp8zdVSmUDr6rEYF82deUyr9EIipjcnmFPzDzl\"\n",
    "\n",
    "# Authenticating\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Loading the Positive and Negative Words\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Positive Words\n",
    "file = open('positive-words.txt', 'r')\n",
    "positive_words = file.read().splitlines() \n",
    "\n",
    "# Negative Words\n",
    "file = open('negative-words.txt', 'r')\n",
    "negative_words = file.read().splitlines() \n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Querying Tweets\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Create tracking variables for tweet iteration\n",
    "tweets = []\n",
    "\n",
    "# Topic to be searched for\n",
    "topic = \"facebook\"\n",
    "\n",
    "# Get and store the 100 latest tweets\n",
    "all_tweets = api.search_tweets(q=f\"{topic} -filter:retweets\", lang=\"en\", count=100)\n",
    "all_tweets = [tweet.text.lower() for tweet in all_tweets]\n",
    "\n",
    "# 2-d array storing sentiment and emotion\n",
    "sentiment = numpy.empty((100, 2), numpy.str)\n",
    "\n",
    "# Keep track of score of sentiment for source\n",
    "num_pos_tweets = 0\n",
    "num_neg_tweets = 0\n",
    "num_neutral_tweets = 0\n",
    "count = 0\n",
    "emotionScore = {\n",
    "    'sadness':0,\n",
    "    'joy':0,\n",
    "    'fear':0,\n",
    "    'disgust':0,\n",
    "    'anger':0\n",
    "}\n",
    "\n",
    "# Iterate through last 100 tweets\n",
    "for tweet in all_tweets:\n",
    "    key_values = {'version': '2021-08-01', 'text': tweet, 'features':'sentiment,emotion'}\n",
    "    response = requests.get(ibm_url+\"/v1/analyze\", key_values, auth = ('apikey', ibm_api_key))\n",
    "    #print(response.json())\n",
    "    #  Make sure that tweet is able to be analyzed\n",
    "    if response.json()[\"language\"] == \"en\":\n",
    "        # see if sentiment is positive or negative\n",
    "        if response.json()[\"sentiment\"][\"document\"][\"label\"] == \"positive\":\n",
    "            num_pos_tweets += 1\n",
    "            sentiment[count][0] = \"positive\"\n",
    "        else:\n",
    "            num_neg_tweets += 1\n",
    "            sentiment[count][0] = \"negative\"\n",
    "\n",
    "        # determine strongest emotion and store it\n",
    "        sadness = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"sadness\"]\n",
    "        joy = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"joy\"]\n",
    "        fear = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"joy\"]\n",
    "        disgust = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"disgust\"]\n",
    "        anger = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"anger\"]\n",
    "        emotionMax = max(sadness, joy, fear, disgust, anger)\n",
    "        if emotionMax == sadness:\n",
    "            emotionScore['sadness'] = emotionScore['sadness'] + 1\n",
    "            sentiment[count][1] = 'sadness'\n",
    "        elif emotionMax == joy:\n",
    "            emotionScore['joy'] = emotionScore['joy'] + 1\n",
    "            sentiment[count][1] = 'joy'\n",
    "        elif emotionMax == fear:\n",
    "            emotionScore['fear'] = emotionScore['fear'] + 1\n",
    "            sentiment[count][1] = 'fear'\n",
    "        elif emotionMax == disgust:\n",
    "            emotionScore['disgust'] = emotionScore['disgust'] + 1\n",
    "            sentiment[count][1] = 'disgust'\n",
    "        else:\n",
    "            emotionScore['anger'] = emotionScore['anger'] + 1\n",
    "            sentiment[count][1] = 'anger'\n",
    "\n",
    "        count += 1\n",
    "\n",
    "# Once we are done analyzing the tweets, print overall score\n",
    "print(f\"{'Tweet Analysis:':<15} Score: {num_pos_tweets-num_neg_tweets:<4} Positive Tweets: {num_pos_tweets:<4} Negative Tweets: {num_neg_tweets:<4} Total Tweets: {count}\")\n",
    "print(emotionScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Stored!\n",
      "142\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# Querying Reddit\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Reddit API Key Data\n",
    "my_client_id = \"hO52jacJFobA_5P5KyIw8A\"\n",
    "my_client_secret = \"KHTg5c1aqtih_tj2qBjUdRM0GV9qiQ\"\n",
    "my_user_agent = \"Scraping ISA414\"\n",
    "\n",
    "# CNBC Article\n",
    "cnbcArticle = \"qht57x\"\n",
    "# CNN Article\n",
    "cnnArticle = \"quv6cr\"\n",
    "# Verge Article\n",
    "vergeArticle = \"qjbmtn\"\n",
    "# IGN  Article\n",
    "ignArticle = \"qmij5v\"\n",
    "# Business Insider Article\n",
    "biArticle = \"qhweln\"\n",
    "# Associated Press Article\n",
    "apArticle = \"qla1aw\"\n",
    "# NBC News Article\n",
    "nbcArticle = \"qht8mn\"\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id = my_client_id,\n",
    "    client_secret = my_client_secret,\n",
    "    user_agent = my_user_agent,\n",
    ")\n",
    "\n",
    "# Define the reddit scraping method\n",
    "def redditScraping(redditAPI, url):\n",
    "\n",
    "    # Variables\n",
    "    count = 0\n",
    "    topcomments = []\n",
    "    submission = reddit.submission(url)\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    # Loop through the top comments and grab the text\n",
    "    for top_level_comment in submission.comments:\n",
    "        topcomments.append(top_level_comment.body)\n",
    "    \n",
    "    return topcomments\n",
    "\n",
    "\n",
    "# Sources\n",
    "companies = [\"CNBC\", \"CNN\", \"The Verge\", \"IGN\", \"Business Insider\", \"Associated Press\", \"NBC News\"]\n",
    "\n",
    "sources_dict = dict.fromkeys(companies, [])\n",
    "\n",
    "\n",
    "# Save the comments  \n",
    "sources_dict[\"CNBC\"].append(redditScraping(reddit, cnbcArticle))\n",
    "sources_dict[\"CNN\"].append(redditScraping(reddit, cnnArticle))\n",
    "sources_dict[\"The Verge\"].append(redditScraping(reddit, vergeArticle))\n",
    "sources_dict[\"IGN\"].append(redditScraping(reddit, ignArticle))\n",
    "sources_dict[\"Business Insider\"].append(redditScraping(reddit, biArticle))\n",
    "sources_dict[\"Associated Press\"].append(redditScraping(reddit, apArticle))\n",
    "sources_dict[\"NBC News\"].append(redditScraping(reddit, nbcArticle))\n",
    "\n",
    "# Print the Dict\n",
    "print(\"Data Stored!\")\n",
    "print(len(sources_dict[\"CNN\"][0][0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Reddit Sentiment & Emotion\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Keep track of score of sentiment for source\n",
    "num_pos_reddit = 0\n",
    "num_neg_reddit = 0\n",
    "num_neutral_reddit = 0\n",
    "count = 0\n",
    "ibmpos = 0\n",
    "ibmneg = 0\n",
    "emotionScore = {\n",
    "    'sadness':0,\n",
    "    'joy':0,\n",
    "    'fear':0,\n",
    "    'disgust':0,\n",
    "    'anger':0\n",
    "}\n",
    "\n",
    "# Iterate through the reddit data\n",
    "\n",
    "# for source in sources_dict:\n",
    "#     for data in source:\n",
    "#         key_values = {'version': '2021-08-01', 'text': data, 'features':'sentiment,emotion'}\n",
    "#         response = requests.get(ibm_url+\"/v1/analyze\", key_values, auth = ('apikey', ibm_api_key))\n",
    "        \n",
    "#         # make sure that tweet is analyzable\n",
    "#         if response.json()[\"language\"] == \"en\":\n",
    "#             # see if sentiment is positive or negative\n",
    "#             if response.json()[\"sentiment\"][\"document\"][\"label\"] == \"positive\":\n",
    "#                 num_pos_reddit += 1\n",
    "#             else:\n",
    "#                 num_neg_reddit += 1\n",
    "\n",
    "#             # determine strongest emotion\n",
    "#             sadness = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"sadness\"]\n",
    "#             joy = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"joy\"]\n",
    "#             fear = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"joy\"]\n",
    "#             disgust = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"disgust\"]\n",
    "#             anger = response.json()[\"emotion\"][\"document\"][\"emotion\"][\"anger\"]\n",
    "#             emotionMax = max(sadness, joy, fear, disgust, anger)\n",
    "#             if emotionMax == sadness:\n",
    "#                 emotionScore['sadness'] = emotionScore['sadness'] + 1\n",
    "#             elif emotionMax == joy:\n",
    "#                 emotionScore['joy'] = emotionScore['joy'] + 1\n",
    "#             elif emotionMax == fear:\n",
    "#                 emotionScore['fear'] = emotionScore['fear'] + 1\n",
    "#             elif emotionMax == disgust:\n",
    "#                 emotionScore['disgust'] = emotionScore['disgust'] + 1\n",
    "#             else:\n",
    "#                 emotionScore['anger'] = emotionScore['anger'] + 1\n",
    "\n",
    "#             count += 1\n",
    "\n",
    "# # Print the results\n",
    "# print(f\"{'Tweet Analysis:':<15} Score: {num_pos_reddit-num_neg_reddit:<4} Positive Tweets: {num_pos_reddit:<4} Negative Tweets: {num_neg_reddit:<4} Total Tweets: {count}\")\n",
    "# print(emotionScore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Outputting Data\n",
    "# ---------------------------------------------\n",
    "\n",
    "# --------------------------------------------- Twitter\n",
    "\n",
    "output = [\"Test1\", \"Test2\"]\n",
    "# File name for the data\n",
    "twitterFile = 'twitter_data.csv'\n",
    "\n",
    "# Delete current CSV file, if it exists\n",
    "if(os.path.exists(twitterFile) and os.path.isfile(twitterFile)):\n",
    "  os.remove(twitterFile)\n",
    "\n",
    "# Writing to csv File\n",
    "header = [\"Tweet\", \"Sentiment\", \"Emotion\"]\n",
    "\n",
    "# Open the CSV to write to\n",
    "with open(twitterFile, 'w', encoding='UTF8', newline='') as f:\n",
    "    # Create the csv writer\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # Create the header\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # Write the data\n",
    "    for i in range(0, len(all_tweets)):\n",
    "      writer.writerow([all_tweets[i], sentiment[i][0], sentiment[i][1]])\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------- Reddit\n",
    "\n",
    "# File name for the data\n",
    "redditFile = 'reddit_data.csv'\n",
    "\n",
    "# Delete current CSV file, if it exists\n",
    "if(os.path.exists(redditFile) and os.path.isfile(redditFile)):\n",
    "  os.remove(redditFile)\n",
    "\n",
    "\n",
    "# Writing to CSV file\n",
    "header = [\"News Source\", \"Tweet\"]\n",
    "\n",
    "# Open the CSV to write to\n",
    "with open(redditFile, 'w', encoding='UTF8', newline='') as f:\n",
    "    # Create the csv writer\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # Create the header\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # Write the data\n",
    "    for key, val in sources_dict.items():\n",
    "      for i in val: \n",
    "        writer.writerow([key, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Analyzing Data\n",
    "# ---------------------------------------------\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
